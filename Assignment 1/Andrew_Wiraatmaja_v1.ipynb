{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e4ecf2",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23184dc3",
   "metadata": {},
   "source": [
    "Firstly, we import several modules that will be used for the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c3f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309cc28",
   "metadata": {},
   "source": [
    "As scraping data from website will be tedious, we create function to get the soup easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912db3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    soup_source = requests.get(url).text\n",
    "    \n",
    "    return BeautifulSoup(soup_source,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8251a8",
   "metadata": {},
   "source": [
    "Then, we will go to DR NTU website and list out professors that are in the list and try to scrape several information from there. However, when we look at the website, the list of professors are listed in two page. There is an idea to automate the process by finding the \"next\" reference, but looks like the reference page is not the same so we will do it in primitive way, which by plug in the website one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4e1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_list_URL1 = \"https://dr.ntu.edu.sg/simple-search?filterquery=ou00030&filtername=school&filtertype=authority&location=researcherprofiles&sort_by=bi_sort_4_sort&order=ASC\"\n",
    "prof_list_URL2 = \"https://dr.ntu.edu.sg/simple-search?query=&location=researcherprofiles&filter_field_1=school&filter_type_1=authority&filter_value_1=ou00030&crisID=&relationName=&sort_by=bi_sort_4_sort&order=asc&rpp=50&etal=0&start=50\"\n",
    "\n",
    "soup_prflist1 = get_soup(prof_list_URL1)\n",
    "soup_prflist2 = get_soup(prof_list_URL2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96babc",
   "metadata": {},
   "source": [
    "Looks like from this we can get two more information which is <b> DR NTU url </b> and <b> Email </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bde668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = [x.text for x in soup_prflist1.find(\"div\", class_=\"discovery-result-results\").find_all('a')][1:]\n",
    "links1 = [\"https://dr.ntu.edu.sg\" + x.get('href') for x in soup_prflist1.find(\"div\", class_=\"discovery-result-results\").find_all('a')][1:]\n",
    "emails1 = [x.text for x in soup_prflist1.find_all(\"td\", headers=\"t3\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45ea0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names2 = [x.text for x in soup_prflist2.find(\"div\", class_=\"discovery-result-results\").find_all('a')][1:]\n",
    "links2 = [\"https://dr.ntu.edu.sg\" + x.get('href') for x in soup_prflist2.find(\"div\", class_=\"discovery-result-results\").find_all('a')][1:]\n",
    "emails2 = [x.text for x in soup_prflist2.find_all(\"td\", headers=\"t3\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d99837",
   "metadata": {},
   "source": [
    "Next, we append the name together the two website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd63eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = names1 + names2\n",
    "links = links1 + links2\n",
    "emails = emails1 + emails2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535bf499",
   "metadata": {},
   "source": [
    "For Website URL, we will use the one that listed in his/her DR-NTU URL. To do this, firstly we have to go to his/her website and scrape the personal website url, which located beside the profile picture under name \"Personal Website\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14780d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "website = []\n",
    "\n",
    "for url in links:\n",
    "    soup_tmp = get_soup(url)\n",
    "    soup_tmp = get_soup(url)\n",
    "    if soup_tmp.find(\"div\",id = \"personalsiteDiv\"):\n",
    "        webs = [x.get('href') for x in soup_tmp.find(\"div\",id = \"personalsiteDiv\").find_all('a')][0]\n",
    "    else:\n",
    "        webs = np.nan\n",
    "    website.append(webs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06eb66",
   "metadata": {},
   "source": [
    "Next, we will lookout for the DBLP URL and Citations from Google Scholar. But firstly, we need to modify some of professors name because some of them have different formatting of name for DBLP and Google Scholar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037078ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_change = {'Jagath Chandana Rajapakse' : 'Jagath C Rajapakse',\n",
    "              'Lana Obraztsova' : 'Svetlana Obraztsova',\n",
    "              'Ke Yiping, Kelly' : 'Ke Yiping',\n",
    "              'Sourav Saha Bhowmick' : 'Sourav S Bhowmick',\n",
    "              'Tay Kian Boon' : 'Tay K Boon',\n",
    "              'Douglas Leslie Maskell' : 'Douglas L Maskell',\n",
    "              'Joty Shafiq Rayhan' : 'Joty Shafiq',\n",
    "              'Luke Ong （翁之昊）': 'Luke Ong',\n",
    "              'Luo Siqiang （骆思强）' : 'Luo Siqiang',\n",
    "              'Quek Hiok Chai' : 'Quek Chai'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12257f72",
   "metadata": {},
   "source": [
    "For DBLP URL, we will use the search to find out the website. When we search we look out for the professor's profile that will be seen on the left one. Then we take the website which is the one that it refers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9f2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dblp = []\n",
    "\n",
    "for name in names:\n",
    "    if name in prof_change.keys():\n",
    "        name_tmp = prof_change[name]\n",
    "        name = name_tmp\n",
    "    name1 = name.replace(\",\",\"\")\n",
    "    name2 = name1.replace(\" \",\"+\")\n",
    "    url = \"https://dblp.org/search?q=\" + name2\n",
    "    soup_tmp = get_soup(url)\n",
    "    if soup_tmp.find(\"ul\",class_ = \"result-list\"):\n",
    "        url_prof = [x.get('href') for x in soup_tmp.find(\"ul\",class_ = \"result-list\").find_all('a')][0]\n",
    "    else:\n",
    "        url_prof = np.nan\n",
    "    url_dblp.append(url_prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea48ea3",
   "metadata": {},
   "source": [
    "For the Google Scholar, to find the profile is the same and next, after we go to the profile we look out for the table on top right of the page and take the number of Citations from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec6815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = []\n",
    "\n",
    "for name in names:\n",
    "    if name in prof_change.keys():\n",
    "        name_tmp = prof_change[name]\n",
    "        name = name_tmp\n",
    "    name1 = name.replace(\",\",\"\")\n",
    "    name2 = name1.replace(\" \",\"+\")\n",
    "    \n",
    "    url = \"https://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors=\" + name2 + \"&btnG=\"\n",
    "    soup_tmp1 = get_soup(url)\n",
    "    if soup_tmp1.find(\"div\",class_ = \"gs_ai_t\"):\n",
    "        url_prof = \"https://scholar.google.com\" + [x.get('href') for x in soup_tmp1.find(\"div\",class_ = \"gs_ai_t\").find_all('a')][0]\n",
    "        soup_tmp2 = get_soup(url_prof)\n",
    "        table = soup_tmp2.find(\"table\", id = \"gsc_rsb_st\")\n",
    "        row = table.find_all(\"tr\")[1]\n",
    "        col = row.find_all(\"td\")\n",
    "        col = [ele.text.strip() for ele in col]\n",
    "        cit = int(col[1])\n",
    "    else:\n",
    "        cit = np.nan\n",
    "    citations.append(cit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241c5ea",
   "metadata": {},
   "source": [
    "Now let's put it together all of them into one dataframe and save it to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ff9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDict = {}\n",
    "myDict['Full Name'] = names\n",
    "myDict['Email'] = emails\n",
    "myDict['DR-NTU URL'] = links\n",
    "myDict['Website URL'] = website\n",
    "myDict['DBLP URL'] = url_dblp\n",
    "myDict['Citations (All)'] = citations\n",
    "\n",
    "df = pd.DataFrame(myDict)\n",
    "df.to_csv(\"Andrew_Wiraatmaja.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee32fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
